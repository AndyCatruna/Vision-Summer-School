{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPskeUpFYij1DMSHr/n+e5+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndyCatruna/Vision-Summer-School/blob/main/Lab02_ConvNets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks"
      ],
      "metadata": {
        "id": "2Ic3_yaPXAGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab we will explore Convolutional Neural Networks for image classification."
      ],
      "metadata": {
        "id": "caq8NyfjQExs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary imports"
      ],
      "metadata": {
        "id": "fn7EC7RyQVoj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kwkdapgczgya"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the CIFAR10 dataset which contains 32x32 images of 10 different classes."
      ],
      "metadata": {
        "id": "FtfR_-lEQYYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO for Exercise 3 - Add more transforms that act as image augmentations\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=256,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=256,\n",
        "                                         shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "GwG1jhP30rty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot some images from the dataset in order to get an idea of how they look."
      ],
      "metadata": {
        "id": "Zh-791w-QlML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images[:8]))\n",
        "# print labels\n",
        "print(' '.join('%8s' % classes[labels[j]] for j in range(8)))"
      ],
      "metadata": {
        "id": "VBN-hjoP08m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the device on which we will train the models. For this lab you should have GPU execution enabled. Training the models on cpu will take too much time.\n",
        "\n",
        "If the models still take too much time to train, you can try to increase the batch size value in the dataloaders (if you increase it too much you might get \"Out of memory error\")."
      ],
      "metadata": {
        "id": "O52Bxme4RWC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "LUbV-2Ok7IP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a training epoch in which we iterate through all the data in the training set and update the model."
      ],
      "metadata": {
        "id": "GivHRLgVRfgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, device, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "    for step, (images, labels) in bar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(images)\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_train_loss / dataset_size, 2)\n",
        "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss)\n",
        "\n",
        "\n",
        "    return epoch_loss"
      ],
      "metadata": {
        "id": "SIJHpfna3Ygn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the validation function in which the model is tested on unseen images."
      ],
      "metadata": {
        "id": "4fI8_mAqRn0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_epoch(model, dataloader, device, criterion, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    total_val_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "    for step, (images, labels) in bar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        pred = model(images)\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        total_val_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_val_loss / dataset_size, 2)\n",
        "\n",
        "        accuracy = np.round(100 * correct / dataset_size, 2)\n",
        "\n",
        "        bar.set_postfix(Epoch=epoch, Valid_Acc=accuracy, Valid_Loss=epoch_loss)\n",
        "\n",
        "    return accuracy, epoch_loss"
      ],
      "metadata": {
        "id": "q4t64YbA3n1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the function that receives the model and trains it with an optimizer and loss function (criterion) for a number of epochs."
      ],
      "metadata": {
        "id": "rEWiymhnRwTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(model, criterion, optimizer, num_epochs):\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "\n",
        "    top_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss = train_epoch(model, trainloader, device, optimizer, criterion, epoch)\n",
        "        with torch.no_grad():\n",
        "            val_accuracy, val_loss = valid_epoch(model, testloader, device, criterion, epoch)\n",
        "            if val_accuracy > top_accuracy:\n",
        "                print(f\"Validation Accuracy Improved ({top_accuracy} ---> {val_accuracy})\")\n",
        "                top_accuracy = val_accuracy\n",
        "        print()"
      ],
      "metadata": {
        "id": "exy1VgkI7BRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fully Connected Neural Network"
      ],
      "metadata": {
        "id": "Q90lMpNCNlxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first train a fully connected network on the images to compare it with the following CNNs. The fully connected neural network is not a very wise choice for images because we lose spatial information."
      ],
      "metadata": {
        "id": "VkHjNtP-C5TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FCNNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCNNet, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=3 * 32 * 32, out_features=360)\n",
        "        self.fc2 = nn.Linear(in_features=360, out_features=120)\n",
        "        self.fc3 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.fc4 = nn.Linear(in_features=84, out_features=10)\n",
        "\n",
        "        # Activation function - can be replaced by other better functions (GELU, Mish etc.)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3 * 32 * 32)\n",
        "\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "\n",
        "        # Output is 10 scores, one for each class\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "iVIPlzH6Npcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "model = FCNNet()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam is an improved gradient descent algorithm\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "a9R1IeDkONzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(model, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "id": "oFy85eqhOYyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Custom Convolutional Network"
      ],
      "metadata": {
        "id": "2oDzBX1h-o7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first construct a custom convolutional network. We use [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) to define a convolutional layer and [MaxPool2D](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html).\n",
        "\n",
        "Notice how the shape of the input **x** changes through convolutional layers and pooling layers. Convolution modifies the channel dimension based on the `out_channels` argument and the spatial dimension based on the dimension of the `kernel_size`. The spatial dimension of the output can also be modified based on the `padding` and `stride` arguments.\n",
        "\n",
        "The spatial dimension is computed with this formula: [(Wâˆ’K+2P)/S]+1.\n",
        "\n",
        "* W is the input size - 32 (in first conv layer)\n",
        "* K is the Kernel size - 5\n",
        "* P is the padding - default 0\n",
        "* S is the stride - default 1\n",
        "\n",
        "You can check the full formula [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) in the 'Shape' section."
      ],
      "metadata": {
        "id": "KisbaSWgSJey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # First conv layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
        "\n",
        "        # Second conv layer\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "\n",
        "        # Pooling layer - used after both layers of convolution\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Linear layers of the fully connected classifier\n",
        "        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
        "\n",
        "        # Activation function - can be replaced by other better functions (GELU, Mish etc.)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional Part\n",
        "\n",
        "        # input - x shape - (3,32,32)\n",
        "        x = self.activation(self.conv1(x))\n",
        "        # After conv - x shape - (6,28,28)\n",
        "\n",
        "        x = self.pool(x)\n",
        "        # After pooling - x shape - (6,14,14)\n",
        "\n",
        "        x = self.activation(self.conv2(x))\n",
        "        # After conv - x shape - (16,10,10)\n",
        "\n",
        "        x = self.pool(x)\n",
        "        # After pooling - x shape - (16,5,5)\n",
        "\n",
        "        # Flatten features for input to fully connected layers\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "\n",
        "        # Fully connected part\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "\n",
        "        # Output is 10 scores, one for each class\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "iWqefchT35g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1** - Run the training. Play with hyperparameters and network structure and try to improve results"
      ],
      "metadata": {
        "id": "F1jSyu-f_B8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "model = ConvNet()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam is an improved gradient descent algorithm\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "G5NTURlo7Xjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(model, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "id": "emQNZkNN7wBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Off the shelf CNN - ResNet18"
      ],
      "metadata": {
        "id": "YAJ00aZN-sO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/XwcnU5x.png\" width=600px>"
      ],
      "metadata": {
        "id": "Ki2gdbHfl4eF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do not need to manually construct new CNN models. There are available architectures in the [torchvision library](https://pytorch.org/vision/0.8/models.html) and in the [timm library](https://timm.fast.ai/). We can modify their output layers to fit our data and train them for our specific task.\n",
        "\n",
        "For this example we use ResNet18 which is a lightweight version of ResNet. We modify the final layer to work with only 10 classes."
      ],
      "metadata": {
        "id": "c1Hyo6ILU8kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18()\n",
        "\n",
        "# ResNet is trained on ImageNet - 1000 classes. We modify the last layer to have only 10 classes\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features=in_features, out_features=10, bias=True)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "GQiCngyH9Cwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2** - Run the training. Play with hyperparameters and try to improve results."
      ],
      "metadata": {
        "id": "4C-30cza_OuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(model, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "id": "2pBKkQYe90U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ResNet18 with weights pretrained on ImageNet"
      ],
      "metadata": {
        "id": "FdoEi9Hn-1Xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do not need to train the model from scratch. We can use weights of models pretrained on large datasets and fine-tune them for our task.\n",
        "\n",
        "Doing this can significantly improve the final accuracy, especially when we have a small amount of training data.\n",
        "\n",
        "In this example we start from weights of a ResNet18 pretrained on ImageNet (1M images). We can see that the model performs much better.\n",
        "\n",
        "However, starting from ImageNet pretrained weights is not allowed in the competition ðŸ˜‰"
      ],
      "metadata": {
        "id": "JkfP0dfzV2W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18(pretrained=True)\n",
        "\n",
        "# ResNet is trained on ImageNet - 1000 classes. We modify the last layer to have only 10 classes\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features=in_features, out_features=10, bias=True)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "s_GpzmDS-S3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3**\n",
        "1. Run the training. Play with hyperparameters and try to improve results.\n",
        "\n",
        "2. Play with transforms for the trainset and add [image augmentations](https://pytorch.org/vision/main/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py). See if they improve results. Do not use augmentations for testset."
      ],
      "metadata": {
        "id": "HffX6yk1_XY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(model, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "id": "_9IrUc5s-WGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vision Transformers"
      ],
      "metadata": {
        "id": "OJKjCqwDP8N_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png\" width=400px>"
      ],
      "metadata": {
        "id": "Op5Z4XQ3mAAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vision Transformers are a new class of image processing architectures. We want to see how these architectures compare to CNNs.\n",
        "\n",
        "For this, we use the [timm](https://timm.fast.ai/) library which provides various computer vision models."
      ],
      "metadata": {
        "id": "O04JEkItBiYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install the library"
      ],
      "metadata": {
        "id": "b95vG83QCBBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "fhVJSA3nSiVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm"
      ],
      "metadata": {
        "id": "2NjZydBQv0NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see all the models timm library provides. (maybe some of them can obtain better performance in the competition ðŸ˜‰)"
      ],
      "metadata": {
        "id": "elPcPxLpv2Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timm.list_models()"
      ],
      "metadata": {
        "id": "toIcpV_Kv5iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a vision transformer (small version) trained on 224x224 images from ImageNet with patches of 16x16."
      ],
      "metadata": {
        "id": "nvphCRSBCDs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vit_model = timm.create_model('vit_small_patch16_224', pretrained=True)\n",
        "in_features = vit_model.head.in_features\n",
        "vit_model.head = nn.Linear(in_features=in_features, out_features=10)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "mrgqBOHgP-bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run the training. In general vision transformers outperform CNNs. However, the vision transformer seems to do worse than the ResNet. Why is this?"
      ],
      "metadata": {
        "id": "w-3F74EeCOB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(model, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "id": "NoK9erESTFDr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}